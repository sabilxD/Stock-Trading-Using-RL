{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class Agent(Portfolio):\n",
    "    def __init__(self, state_dim, balance, is_eval=False, model_name=\"\"):\n",
    "        super().__init__(balance=balance)\n",
    "        self.model_type = 'DQN'\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = 3  # hold, buy, sell\n",
    "        self.memory = deque(maxlen=100)\n",
    "        self.buffer_size = 60\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0  # initial exploration rate\n",
    "        self.epsilon_min = 0.01  # minimum exploration rate\n",
    "        self.epsilon_decay = 0.995 # decrease exploration rate as the agent becomes good at trading\n",
    "        self.is_eval = is_eval\n",
    "        self.model = load_model('models/{}.h5'.format(model_name)) if is_eval else self.model()\n",
    "\n",
    "        self.tensorboard = TensorBoard(log_dir='./logs/DQN_tensorboard', update_freq=90)\n",
    "        self.tensorboard.set_model(self.model)\n",
    "\n",
    "\n",
    "    def model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, input_dim=self.state_dim, activation='relu'))\n",
    "        model.add(Dense(units=32, activation='relu'))\n",
    "        model.add(Dense(units=8, activation='relu'))\n",
    "        model.add(Dense(self.action_dim, activation='softmax'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.01))\n",
    "        return model\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_portfolio()\n",
    "        self.epsilon = 1.0 #reset exploration rate\n",
    "\n",
    "    def remember(self, state, actions, reward, next_state, done):\n",
    "        self.memory.append((state, actions, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if not self.is_eval and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        options = self.model.predict(state)\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "\n",
    "    def experience_replay(self):\n",
    "        # retrieve recent buffer_size long memory\n",
    "        mini_batch = [self.memory[i] for i in range(len(self.memory) - self.buffer_size + 1, len(self.memory))]\n",
    "\n",
    "        for state, actions, reward, next_state, done in mini_batch:\n",
    "            if not done:\n",
    "                Q_target_value = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                Q_target_value = reward\n",
    "            next_actions = self.model.predict(state)\n",
    "            next_actions[0][np.argmax(actions)] = Q_target_value\n",
    "            history = self.model.fit(state, next_actions, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return history.history['loss'][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DQN'\n",
    "stock_name = input('Stock Name: ')\n",
    "window_size = 3\n",
    "num_episode = 2\n",
    "initial_balance = 10000\n",
    "\n",
    "stock_prices = stock_close_prices(stock_name)\n",
    "\n",
    "trading_period = len(stock_prices) - 1\n",
    "returns_across_episodes = []\n",
    "num_experience_replay = 0\n",
    "action_dict = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
    "\n",
    "\n",
    "# model = importlib.import_module(f'agents.{model_name}')\n",
    "agent = Agent(state_dim=window_size + 3, balance=initial_balance)\n",
    "\n",
    "\n",
    "\n",
    "def hold(actions):\n",
    "    #encourage selling for profit and liquidity\n",
    "    next_probable_action = np.argsort(actions)[1]\n",
    "    if next_probable_action == 2 and len(agent.inventory) > 0:\n",
    "        max_profit = stock_prices[t] - min(agent.inventory)\n",
    "        if max_profit > 0:\n",
    "            sell(t)\n",
    "            actions[next_probable_action] = 1 # reset this action's value to the highest\n",
    "            return 'Hold', actions\n",
    "\n",
    "def buy(t):\n",
    "    if agent.balance > stock_prices[t]:\n",
    "        agent.balance -= stock_prices[t]\n",
    "        agent.inventory.append(stock_prices[t])\n",
    "        return 'Buy: ${:.2f}'.format(stock_prices[t])\n",
    "\n",
    "def sell(t):\n",
    "    if len(agent.inventory) > 0:\n",
    "        agent.balance += stock_prices[t]\n",
    "        bought_price = agent.inventory.pop(0)\n",
    "        profit = stock_prices[t] - bought_price\n",
    "        global reward\n",
    "        reward = profit\n",
    "        return 'Sell: ${:.2f} | Profit: ${:.2f}'.format(stock_prices[t], profit)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(1 , num_episode+1):\n",
    "    print(f'Episode : {episode}')\n",
    "    \n",
    "    agent.reset() #see\n",
    "    state = generate_combined_state(0, window_size, stock_prices, agent.balance, len(agent.inventory))\n",
    "\n",
    "    for t in range(1, trading_period + 1):\n",
    "        if t % 100 == 0:\n",
    "            print(f'\\n-------------------Period: {t}/{trading_period}-------------------')\n",
    "\n",
    "        reward = 0\n",
    "        next_state = generate_combined_state(t, window_size, stock_prices, agent.balance, len(agent.inventory))\n",
    "        previous_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n",
    "        \n",
    "        actions = agent.model.predict(state)[0]\n",
    "        action = agent.act(state)\n",
    "\n",
    "        print('Step: {}\\tHold signal: {:.4} \\tBuy signal: {:.4} \\tSell signal: {:.4}'.format(t, actions[0], actions[1], actions[2]))\n",
    "\n",
    "        if action != np.argmax(actions): print(f\"'{action_dict[action]}' is an exploration.\")\n",
    "        if action == 0: # hold\n",
    "            execution_result = hold(actions)\n",
    "        if action == 1: # buy\n",
    "            execution_result = buy(t)      \n",
    "        if action == 2: # sell\n",
    "            execution_result = sell(t)        \n",
    "\n",
    "        if isinstance(execution_result, tuple): # if execution_result is 'Hold'\n",
    "                actions = execution_result[1]\n",
    "                execution_result = execution_result[0]\n",
    "                print(execution_result)\n",
    "                \n",
    "        current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n",
    "        unrealized_profit = current_portfolio_value - agent.initial_portfolio_value\n",
    "        reward += unrealized_profit\n",
    "\n",
    "        agent.portfolio_values.append(current_portfolio_value)\n",
    "        agent.return_rates.append((current_portfolio_value - previous_portfolio_value) / previous_portfolio_value)\n",
    "        \n",
    "        \n",
    "        done = True if t == trading_period else False\n",
    "        agent.remember(state, actions, reward, next_state, done)\n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        #experience replay \n",
    "        if len(agent.memory) > agent.buffer_size: #see\n",
    "            num_experience_replay += 1\n",
    "            loss = agent.experience_replay()\n",
    "            print('Episode: {}\\tLoss: {:.2f}\\tAction: {}\\tReward: {:.2f}\\tBalance: {:.2f}\\tNumber of Stocks: {}'.format(episode, loss, action_dict[action], reward, agent.balance, len(agent.inventory)))\n",
    "            agent.tensorboard.on_batch_end(num_experience_replay, {'loss': loss, 'portfolio value': current_portfolio_value}) \n",
    "\n",
    "        if done:\n",
    "            portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n",
    "            returns_across_episodes.append(portfolio_return)\n",
    "\n",
    "        if episode % 2 == 0:\n",
    "            if model_name == 'DQN':\n",
    "                agent.model.save('models/DQN_ep' + str(episode) + '.h5')\n",
    "                \n",
    "print('total training time: {0:.2f} min'.format((time.time() - start_time)/60))\n",
    "plot_portfolio_returns_across_episodes(model_name, returns_across_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_to_load = input('model_to_load: ')\n",
    "stock_name = input('stock_name: ')\n",
    "initial_balance = 10000\n",
    "display = True\n",
    "window_size = 3\n",
    "action_dict = {0: 'Hold', 1: 'Hold', 2: 'Sell'}\n",
    "\n",
    "agent = Agent(state_dim=window_size + 3, balance=initial_balance, is_eval=True, model_name=model_to_load)\n",
    "# select evaluation model\n",
    "\n",
    "\n",
    "\n",
    "portfolio_return = 0\n",
    "while portfolio_return == 0: # a hack to avoid stationary case\n",
    "    stock_prices = stock_close_prices(stock_name)\n",
    "    trading_period = len(stock_prices) - 1\n",
    "    state = generate_combined_state(0, window_size, stock_prices, agent.balance, len(agent.inventory))\n",
    "\n",
    "    for t in range(1, trading_period + 1):\n",
    "        if model_name == 'DDPG':\n",
    "            actions = agent.act(state, t)\n",
    "            action = np.argmax(actions)\n",
    "        else:\n",
    "            actions = agent.model.predict(state)[0]\n",
    "            action = agent.act(state)\n",
    "\n",
    "        # print('actions:', actions)\n",
    "        # print('chosen action:', action)\n",
    "\n",
    "        next_state = generate_combined_state(t, window_size, stock_prices, agent.balance, len(agent.inventory))\n",
    "        previous_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n",
    "        \n",
    "        # execute position\n",
    "        print(f'Step: {t}')\n",
    "        if action != np.argmax(actions): print(f\"\\t\\t'{action_dict[action]}' is an exploration.\")\n",
    "        if action == 0: hold() # hold\n",
    "        if action == 1 and agent.balance > stock_prices[t]: buy(t) # buy\n",
    "        if action == 2 and len(agent.inventory) > 0: sell(t) # sell\n",
    "\n",
    "        current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n",
    "        agent.return_rates.append((current_portfolio_value - previous_portfolio_value) / previous_portfolio_value)\n",
    "        agent.portfolio_values.append(current_portfolio_value)\n",
    "        state = next_state\n",
    "\n",
    "        done = True if t == trading_period else False\n",
    "        if done:\n",
    "            portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n",
    "\n",
    "if display:\n",
    "    # plot_portfolio_transaction_history(stock_name, agent)\n",
    "    # plot_portfolio_performance_comparison(stock_name, agent)\n",
    "    plot_all(stock_name, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
